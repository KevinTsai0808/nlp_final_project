{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cairuikai/Downloads/NLP_project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ckiplab/bert-base-chinese\"\n",
    "data_path = \"data\"\n",
    "batch_size = 32\n",
    "hidden_dim = 384\n",
    "hidden_acti = nn.Tanh()\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 2e-5\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv):\n",
    "    df = pd.read_csv(csv, sep=\"\\t\", encoding=\"utf-8\")\n",
    "    df.drop(\"No.\", axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "def get_max_sentence_len(data_path):\n",
    "    max_tokens = 0\n",
    "    for csv in os.listdir(data_path):\n",
    "        sentences = load_df(os.path.join(data_path, csv))[\"Text\"]\n",
    "        for sentence in sentences:\n",
    "            ids = tokenizer.encode(sentence)\n",
    "            max_tokens = max(max_tokens, len(ids))\n",
    "    return max_tokens\n",
    "\n",
    "\n",
    "max_tokens = get_max_sentence_len(data_path)\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: 研究一下padding要用longest, max_length, or True\n",
    "class ValenceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_tokens):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # encode : transform sentence to vocab no., tokenize : transform sentence to text\n",
    "    def tokenize(self, sentence, max_tokens):\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_tokens,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # one sample per call, so need to sqeeze\n",
    "        input_ids = encoded_dict[\"input_ids\"].squeeze()\n",
    "        token_type_ids = encoded_dict[\"token_type_ids\"].squeeze()\n",
    "        attention_mask = encoded_dict[\"attention_mask\"]\n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.df.iloc[index][\"Text\"]\n",
    "        label = self.df.iloc[index][\"Valence_Mean\"]\n",
    "        input_ids, token_by_ids, attention_mask = self.tokenize(\n",
    "            sentence, self.max_tokens\n",
    "        )\n",
    "        sample = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": torch.tensor(label, dtype=torch.float32),\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ArousalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_tokens):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # encode : transform sentence to vocab no., tokenize : transform sentence to text\n",
    "    def tokenize(self, sentence, max_tokens):\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_tokens,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # one sample per call, so need to sqeeze\n",
    "        input_ids = encoded_dict[\"input_ids\"].squeeze()\n",
    "        token_type_ids = encoded_dict[\"token_type_ids\"].squeeze()\n",
    "        attention_mask = encoded_dict[\"attention_mask\"]\n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.df.iloc[index][\"Text\"]\n",
    "        label = self.df.iloc[index][\"Arousal_Mean\"]\n",
    "        input_ids, token_by_ids, attention_mask = self.tokenize(\n",
    "            sentence, self.max_tokens\n",
    "        )\n",
    "        sample = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": torch.tensor(label, dtype=torch.float32),\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_datasets = []\n",
    "a_datasets = []\n",
    "df_list = [load_df(os.path.join(data_path, csv)) for csv in os.listdir(data_path)]\n",
    "for fold_df in df_list:\n",
    "    v_dataset = ValenceDataset(fold_df, tokenizer, max_tokens)\n",
    "    a_dataset = ArousalDataset(fold_df, tokenizer, max_tokens)\n",
    "    v_datasets.append(v_dataset)\n",
    "    a_datasets.append(a_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CVAT_4_SD.csv',\n",
       " 'CVAT_2_SD.csv',\n",
       " 'CVAT_5_SD.csv',\n",
       " 'CVAT_1_SD.csv',\n",
       " 'CVAT_3_SD.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForRegression(nn.Module):\n",
    "    def __init__(self, pm, hidden_dim, hidden_acti, dropout_rate, output_dim):\n",
    "        super(BERTForRegression, self).__init__()\n",
    "        self.pm = AutoModel.from_pretrained(\n",
    "            pm, output_attentions=False, output_hidden_states=False\n",
    "        )\n",
    "        self.hidden = nn.Linear(self.pm.config.hidden_size, hidden_dim)\n",
    "        self.activation = hidden_acti\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(384, output_dim)\n",
    "    \n",
    "    def forward(self, input_sentence, input_mask):\n",
    "        # [0]:last_hidden_state (batch_size, max_length, 768), [1]:pooling layer\n",
    "        pm_output = self.pm(input_sentence, attention_mask=input_mask)[1] #(batch_size, 768)\n",
    "        hidden = self.hidden(pm_output) \n",
    "        hidden = self.activation(hidden)\n",
    "        hidden = self.dropout(hidden) # (batch_size, 384)\n",
    "        output = self.output_layer(hidden) # (batch_size, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (CNN + BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format time to hh:mm:ss\n",
    "def format_time(time):\n",
    "    time_round = int(round((time)))\n",
    "    return str(datetime.timedelta(seconds=time_round))\n",
    "\n",
    "def to_excel(save_string, training_stats):\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    df_stats = df_stats.set_index('Fold')\n",
    "    df_stats.to_csv(save_string, index=False)\n",
    "    return df_stats\n",
    "\n",
    "# save model to path\n",
    "def save_checkpoint(save_path, model):\n",
    "    if save_path == None:\n",
    "        return\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "# load model from path\n",
    "def load_checkpoint(load_path, model, device):\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'\\nModel loaded from <== {load_path}')\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trainer(model, fold_index, train_loader, val_loader, training_stats):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    # MAE\n",
    "    loss_function = torch.nn.L1Loss()\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    for epoch in range(epochs):\n",
    "    # for epoch in range(5):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        total_val_outputs = []\n",
    "        total_val_labels = []\n",
    "        print(f\"\\n---------------EPOCH {epoch+1}-----------------\\n\")\n",
    "        # print(f\"\\n---------------EPOCH {epoch+16}-----------------\\n\")\n",
    "        model.train()\n",
    "        # model.eval()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                pass_time = format_time(time.time() - start_time)\n",
    "                print(f'\\nComplete Batch {step}  of  {len(train_loader)}.   Time: {pass_time}\\n')\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            # with torch.no_grad():\n",
    "            #     outputs = model(input_ids, attention_mask)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_function(outputs.view(-1), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "            # gradient descent\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        training_time = format_time(time.time() - start_time)\n",
    "        \n",
    "        print(f\"\\nTraining loss: {avg_train_loss}\\n\")\n",
    "        print(f\"\\nTraining time: {training_time}\\n\")\n",
    "        print(f\"\\nValidation ........\\n\")\n",
    "        valid_start_time = time.time()\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_function(outputs.view(-1), labels.view(-1))\n",
    "            total_valid_loss += loss.item()\n",
    "\n",
    "            total_val_outputs = total_val_outputs + outputs.view(-1).detach().cpu().numpy().tolist()\n",
    "            total_val_labels = total_val_labels + labels.view(-1).detach().cpu().numpy().tolist()\n",
    "\n",
    "        avg_val_loss = total_valid_loss / len(val_loader)\n",
    "        validation_time = format_time(time.time() - valid_start_time)\n",
    "\n",
    "        print(f\"\\nValidation Loss: {avg_val_loss}\\n\")\n",
    "        print(f\"\\nValidation time: {validation_time}\\n\")\n",
    "        # \n",
    "        # break\n",
    "    # print(total_val_outputs)\n",
    "    # print(total_val_labels)\n",
    "    val_r, _ = pearsonr(total_val_outputs, total_val_labels)\n",
    "\n",
    "    print(f'\\nFinal validation correlation: {val_r}\\n')\n",
    "    training_stats.append(\n",
    "    {\n",
    "        'Fold': fold_index+1,\n",
    "        'Training Loss': avg_train_loss,\n",
    "        'Valid Loss': avg_val_loss,\n",
    "        'Valid correlation' : val_r\n",
    "    })\n",
    "    \n",
    "    print(\"\\nTraining complete!\\n\")\n",
    "    print(f\"\\nTotal training took {format_time(time.time()-total_t0)} (hh:mm:ss)\")\n",
    "    save_string = f'bert-base-chinese' + '_'+f'{fold_index+1}' + '_' + 'V'+ '_' + '0503' + '.csv'\n",
    "    return training_stats, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========Training fold 3===========\n",
      "\n",
      "TrainDataLoader contains 2376 samples\n",
      "Sample data from the first batch:\n",
      "Input IDs: torch.Size([32, 214])\n",
      "Attention Mask: torch.Size([32, 1, 214])\n",
      "Label: torch.Size([32])\n",
      "Example sentence: tensor([ 101,  671, 7274, 1993, 3976, 3173, 2023, 1372, 3221, 3382, 2595, 2867,\n",
      "        5179, 6857,  763, 6206, 3724, 8024, 2361, 3307, 2042, 2527, 1086, 2537,\n",
      "        7269, 6243, 6359, 8024, 3760, 2682, 1168, 2849, 2113, 2552, 1147, 4638,\n",
      "        3976, 2038, 2038, 4684, 2970, 7300, 1062, 1385, 8024, 6206, 3724, 1961,\n",
      "        4989, 1174, 6894, 1139, 6798, 1439, 8024,  699,  684, 1762, 4707, 4680,\n",
      "        4727, 4727,  678, 2200, 1961, 2870, 1343, 7015, 7368, 2970, 1358,  978,\n",
      "        2434, 3596, 3389,  511,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Example label: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------EPOCH 1-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 1.3746547444661459\n",
      "\n",
      "\n",
      "Training time: 0:01:40\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.8107217863986367\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 2-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.8365668924649556\n",
      "\n",
      "\n",
      "Training time: 0:01:43\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.8192062095591897\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 3-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.7908128380775452\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.8097955553155196\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 4-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:56\n",
      "\n",
      "\n",
      "Training loss: 0.7467973426977793\n",
      "\n",
      "\n",
      "Training time: 0:01:44\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.733122235850284\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 5-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:58\n",
      "\n",
      "\n",
      "Training loss: 0.7149691979090372\n",
      "\n",
      "\n",
      "Training time: 0:01:48\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6818253523425052\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 6-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:59\n",
      "\n",
      "\n",
      "Training loss: 0.6858497428894043\n",
      "\n",
      "\n",
      "Training time: 0:01:48\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7917960819445158\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 7-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:56\n",
      "\n",
      "\n",
      "Training loss: 0.6409412721792856\n",
      "\n",
      "\n",
      "Training time: 0:01:43\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7306950311911734\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 8-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.5850864183902741\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6691403200751856\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 9-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.5711689555644989\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.73585507744237\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 10-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.5007835630575815\n",
      "\n",
      "\n",
      "Training time: 0:01:43\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7514342289221915\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 11-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:57\n",
      "\n",
      "\n",
      "Training loss: 0.4745253145694733\n",
      "\n",
      "\n",
      "Training time: 0:01:45\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7257121205329895\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 12-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:57\n",
      "\n",
      "\n",
      "Training loss: 0.44406850695610045\n",
      "\n",
      "\n",
      "Training time: 0:01:46\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7659013490927847\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 13-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:57\n",
      "\n",
      "\n",
      "Training loss: 0.42845486402511596\n",
      "\n",
      "\n",
      "Training time: 0:01:46\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6619016935950831\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 14-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.3925371209780375\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7150288977121052\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "---------------EPOCH 15-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  75.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.3614799972375234\n",
      "\n",
      "\n",
      "Training time: 0:01:43\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7071551366856224\n",
      "\n",
      "\n",
      "Validation time: 0:00:07\n",
      "\n",
      "\n",
      "Final validation correlation: 0.569665744698627\n",
      "\n",
      "\n",
      "Training complete!\n",
      "\n",
      "\n",
      "Total training took 0:27:47 (hh:mm:ss)\n",
      "Model saved to ==> checkpoint/bert-base-chinese_3_A_0504.pt\n",
      "\n",
      "========Training fold 4===========\n",
      "\n",
      "TrainDataLoader contains 2360 samples\n",
      "Sample data from the first batch:\n",
      "Input IDs: torch.Size([32, 214])\n",
      "Attention Mask: torch.Size([32, 1, 214])\n",
      "Label: torch.Size([32])\n",
      "Example sentence: tensor([ 101, 2349, 4273, 7032, 4158,  671,  943, 4028, 6303, 3976,  991, 4638,\n",
      "        6341, 4943, 1058, 2990, 1168, 8024, 2792, 1762, 6749, 1281, 1058, 4638,\n",
      "         821, 3511, 6291, 4158, 8024, 5195, 4089, 6134, 4412, 4952,  978,  852,\n",
      "         679,  778, 7927, 8024,  821, 3511, 4192, 3791, 1217, 1019, 8024, 1348,\n",
      "        2205, 3124, 3780, 1139, 4412, 1060, 3513, 1265, 2697, 1168, 2726, 2719,\n",
      "         511,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Example label: 3.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------EPOCH 1-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:56\n",
      "\n",
      "\n",
      "Training loss: 1.2882628247544572\n",
      "\n",
      "\n",
      "Training time: 0:01:43\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.8646664242995413\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 2-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.8293853587395436\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7534728332569725\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 3-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 0.7546451277024037\n",
      "\n",
      "\n",
      "Training time: 0:01:40\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7387192468894156\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 4-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.6518057516297778\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6963475346565247\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 5-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.5603021435640954\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7285498255177548\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 6-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.49395844743058487\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7511618200101351\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 7-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:56\n",
      "\n",
      "\n",
      "Training loss: 0.4288921352173831\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7700199139745612\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 8-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.39410982764250524\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7328995278007105\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 9-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:56\n",
      "\n",
      "\n",
      "Training loss: 0.3569453894286542\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7901155885897184\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 10-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:56\n",
      "\n",
      "\n",
      "Training loss: 0.3377194203235008\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7724612135636179\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 11-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:57\n",
      "\n",
      "\n",
      "Training loss: 0.31912258529179804\n",
      "\n",
      "\n",
      "Training time: 0:01:44\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.738291301225361\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 12-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:57\n",
      "\n",
      "\n",
      "Training loss: 0.280123075319303\n",
      "\n",
      "\n",
      "Training time: 0:01:46\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7134262511604711\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 13-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.28265721471728505\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7073426183901335\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 14-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.26241933272497076\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7088336944580078\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 15-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.2798542418592685\n",
      "\n",
      "\n",
      "Training time: 0:01:43\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7675597573581495\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "Final validation correlation: 0.530488073177398\n",
      "\n",
      "\n",
      "Training complete!\n",
      "\n",
      "\n",
      "Total training took 0:27:25 (hh:mm:ss)\n",
      "Model saved to ==> checkpoint/bert-base-chinese_4_A_0504.pt\n",
      "\n",
      "========Training fold 5===========\n",
      "\n",
      "TrainDataLoader contains 2360 samples\n",
      "Sample data from the first batch:\n",
      "Input IDs: torch.Size([32, 214])\n",
      "Attention Mask: torch.Size([32, 1, 214])\n",
      "Label: torch.Size([32])\n",
      "Example sentence: tensor([ 101, 5918, 1385, 7770, 1501, 6549, 7128, 4275, 8024, 6900, 1394, 7770,\n",
      "        2428, 6818, 6213, 8024, 7128, 3427,  818, 6908,  172, 1921, 6506, 4872,\n",
      "        1469, 6213, 4518, 4706, 7128, 2201, 4245, 2421, 1060, 4275, 1545, 1019,\n",
      "        8966, 8157, 1039, 8024,  966, 1351, 1377, 7526, 1357, 8360, 8157, 1039,\n",
      "        1032, 2669, 1171, 8024,  775, 1358, 1168, 2797, 1019, 8195, 1039,  123,\n",
      "        4275, 8024, 1599, 3631, 4638,  966, 1351,  679, 6206, 7097, 6882,  511,\n",
      "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Example label: 4.111000061035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------EPOCH 1-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:01:01\n",
      "\n",
      "\n",
      "Training loss: 1.2912166899925954\n",
      "\n",
      "\n",
      "Training time: 0:01:49\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.8612936390073676\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 2-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.824120828428784\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.8158869021817258\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 3-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 0.7282856090648754\n",
      "\n",
      "\n",
      "Training time: 0:01:40\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7300203850394801\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 4-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.6308715693853997\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.8165470706789117\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 5-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 0.5320732363977948\n",
      "\n",
      "\n",
      "Training time: 0:01:40\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6992139094754269\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 6-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.4645576219300966\n",
      "\n",
      "\n",
      "Training time: 0:01:40\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7038327612374958\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 7-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.40150148280569026\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.699167107280932\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 8-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.3723521141989811\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7383386586841784\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 9-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:55\n",
      "\n",
      "\n",
      "Training loss: 0.3346191165415016\n",
      "\n",
      "\n",
      "Training time: 0:01:42\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.725002015891828\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 10-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:58\n",
      "\n",
      "\n",
      "Training loss: 0.3116094411627666\n",
      "\n",
      "\n",
      "Training time: 0:01:48\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.697897942442643\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 11-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:01:01\n",
      "\n",
      "\n",
      "Training loss: 0.296559489860728\n",
      "\n",
      "\n",
      "Training time: 0:01:48\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6743154306160776\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 12-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 0.30161483585834503\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.7166951612422341\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 13-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 0.27850588310409236\n",
      "\n",
      "\n",
      "Training time: 0:01:40\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6859844515198156\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 14-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 0.253179473651422\n",
      "\n",
      "\n",
      "Training time: 0:01:40\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.6909984883509184\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "---------------EPOCH 15-----------------\n",
      "\n",
      "\n",
      "Complete Batch 40  of  74.   Time: 0:00:54\n",
      "\n",
      "\n",
      "Training loss: 0.2428236704420399\n",
      "\n",
      "\n",
      "Training time: 0:01:41\n",
      "\n",
      "\n",
      "Validation ........\n",
      "\n",
      "\n",
      "Validation Loss: 0.677827985663163\n",
      "\n",
      "\n",
      "Validation time: 0:00:08\n",
      "\n",
      "\n",
      "Final validation correlation: 0.623572621443881\n",
      "\n",
      "\n",
      "Training complete!\n",
      "\n",
      "\n",
      "Total training took 0:27:27 (hh:mm:ss)\n",
      "Model saved to ==> checkpoint/bert-base-chinese_5_A_0504.pt\n"
     ]
    }
   ],
   "source": [
    "# train for Arousal\n",
    "for fold_index, dataset in enumerate(a_datasets):\n",
    "    if fold_index == 0:\n",
    "        continue\n",
    "    if fold_index == 1:\n",
    "        continue\n",
    "    print(f\"\\n========Training fold {fold_index + 1}===========\\n\")\n",
    "    train_loaders = [\n",
    "        data for index, data in enumerate(a_datasets) if index != fold_index\n",
    "    ]\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # concat multiple dataloader to oen dataloader\n",
    "    train_loader = torch.utils.data.ConcatDataset(train_loaders)\n",
    "    train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f\"TrainDataLoader contains {len(train_loader.dataset)} samples\")\n",
    "    batch = next(iter(train_loader))\n",
    "    print(\"Sample data from the first batch:\")\n",
    "    print(\"Input IDs:\", batch[\"input_ids\"].shape)  # (batch_size, max_length)\n",
    "    print(\"Attention Mask:\", batch[\"attention_mask\"].shape)  # (batch_size, max_length)\n",
    "    print(\"Label:\", batch[\"label\"].shape)  # (batch_size, )\n",
    "    print(\"Example sentence:\", batch[\"input_ids\"][0])\n",
    "    print(\"Example label:\", batch[\"label\"][0].item())\n",
    "\n",
    "    model = BERTForRegression(model_name, hidden_dim, hidden_acti, dropout_rate, 1)\n",
    "    model.to(device)\n",
    "\n",
    "    # model = load_checkpoint(f'checkpoint/bert-base-chinese_{fold_index+1}_A_0504.pt', model, device)\n",
    "    \n",
    "    # freeze some layers (top | middle | bottom):\n",
    "    # bottom = range(2, 12)\n",
    "    # middle = list(range(0,5))+list(range(7,12))\n",
    "    # top = range(0, 10)\n",
    "    # freeze_layers = top\n",
    "    # for i in freeze_layers:\n",
    "    # #   print(i)\n",
    "    #   for param in model.pm.encoder.layer[i].parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    training_stat, model = Trainer(model, fold_index, train_loader, val_loader, training_stats)\n",
    "    save_checkpoint(os.path.join('checkpoint',f'bert-base-chinese_{fold_index+1}_A_0504.pt'), model)\n",
    "\n",
    "save_string = f\"bert-base-chinese\" + \"_\" + \"A2\" + \"_\" + \"0504\" + \".csv\"\n",
    "# to_excel(save_string, training_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train for Valence\n",
    "# training_stats = []\n",
    "# for fold_index, dataset in enumerate(v_datasets):\n",
    "#     if fold_index == 0:\n",
    "#         continue\n",
    "#     print(f\"\\n========Training fold {fold_index + 1}===========\\n\")\n",
    "#     train_loaders = [\n",
    "#         data for index, data in enumerate(v_datasets) if index != fold_index\n",
    "#     ]\n",
    "#     val_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     # concat multiple dataloader to oen dataloader\n",
    "#     train_loader = torch.utils.data.ConcatDataset(train_loaders)\n",
    "#     train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     print(f\"TrainDataLoader contains {len(train_loader.dataset)} samples\")\n",
    "#     batch = next(iter(train_loader))\n",
    "#     print(\"Sample data from the first batch:\")\n",
    "#     print(\"Input IDs:\", batch[\"input_ids\"].shape)  # (batch_size, max_length)\n",
    "#     print(\"Attention Mask:\", batch[\"attention_mask\"].shape)  # (batch_size, max_length)\n",
    "#     print(\"Label:\", batch[\"label\"].shape)  # (batch_size, )\n",
    "#     print(\"Example sentence:\", batch[\"input_ids\"][0])\n",
    "#     print(\"Example label:\", batch[\"label\"][0].item())\n",
    "\n",
    "#     model = BERTForRegression(model_name, hidden_dim, hidden_acti, dropout_rate, 1)\n",
    "#     model.to(device)\n",
    "\n",
    "#     model = load_checkpoint(f'checkpoint/bert-base-chinese_{fold_index+1}_V_0504.pt', model, device)\n",
    "    \n",
    "#     # freeze some layers (top | middle | bottom):\n",
    "#     # bottom = range(2, 12)\n",
    "#     # middle = list(range(0,5))+list(range(7,12))\n",
    "#     # top = range(0, 10)\n",
    "#     # freeze_layers = top\n",
    "#     # for i in freeze_layers:\n",
    "#     # #   print(i)\n",
    "#     #   for param in model.pm.encoder.layer[i].parameters():\n",
    "#     #     param.requires_grad = False\n",
    "    \n",
    "#     training_stat, model = Trainer(model, fold_index, train_loader, val_loader, training_stats)\n",
    "#     # save_checkpoint(os.path.join('checkpoint',f'bert-base-chinese_{fold_index+1}_V_0504.pt'), model)\n",
    "\n",
    "# save_string = f\"bert-base-chinese\" + \"_\" + \"V\" + \"_\" + \"0504\" + \".csv\"\n",
    "# # to_excel(save_string, training_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
