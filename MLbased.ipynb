{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cairuikai/Downloads/NLP_project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from opencc import OpenCC\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at infgrad/stella-base-zh-v3-1792d and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data'\n",
    "sm = SentenceTransformer('infgrad/stella-base-zh-v3-1792d')\n",
    "# sm = SentenceTransformer(\"aspire/acge_text_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sm, text):\n",
    "    embedding = sm.encode(text)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Text</th>\n",
       "      <th>Valence_Mean</th>\n",
       "      <th>Arousal_Mean</th>\n",
       "      <th>Valence_SD</th>\n",
       "      <th>Arousal_SD</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2761</td>\n",
       "      <td>呂興巨集患有老年癡呆的68歲鄖陽區老人呂興巨集，於5月22日上午從鄖陽區南化塘鎮走失後，一直...</td>\n",
       "      <td>2.889</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.567</td>\n",
       "      <td>1.414</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1736</td>\n",
       "      <td>其次，不願罷免與馬總統的施政表現無直接相關，純粹只是不願意看到罷免案所引發的政治動盪，會讓臺...</td>\n",
       "      <td>3.500</td>\n",
       "      <td>5.143</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.639</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1759</td>\n",
       "      <td>美國官員表示，德黑蘭的正面迴應，暗示今年美伊舉行會談的機會增加。</td>\n",
       "      <td>5.222</td>\n",
       "      <td>5.625</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.696</td>\n",
       "      <td>political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2755</td>\n",
       "      <td>彌月禮盒真的很多種類，光是挑選就眼花撩亂和老公光是吃一堆油飯和蛋糕就吃得好飽後來看到明月堂可...</td>\n",
       "      <td>6.375</td>\n",
       "      <td>4.857</td>\n",
       "      <td>0.484</td>\n",
       "      <td>1.457</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2742</td>\n",
       "      <td>我非常佩服姚明的一點，不僅僅是他苦練球技和體能，用一場場比賽戰績證明自己，更是高超的情商和豁...</td>\n",
       "      <td>7.000</td>\n",
       "      <td>5.111</td>\n",
       "      <td>0.707</td>\n",
       "      <td>2.283</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2009</td>\n",
       "      <td>可愛到爆的皮卡丘和傑尼龜、可達鴨等寶可夢一起在過去20多年裡征服的不僅是全世界的童心，還有大...</td>\n",
       "      <td>5.667</td>\n",
       "      <td>5.200</td>\n",
       "      <td>0.471</td>\n",
       "      <td>1.327</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>2024</td>\n",
       "      <td>5月7日，米家空調伴侶2新品開售，價格79元，讓家中傳統空調變智慧，比169元的1代便宜了一...</td>\n",
       "      <td>6.222</td>\n",
       "      <td>4.333</td>\n",
       "      <td>0.916</td>\n",
       "      <td>1.764</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>673</td>\n",
       "      <td>您的入住滿意就是對我們接待服務工作最好的肯定與激勵！</td>\n",
       "      <td>6.778</td>\n",
       "      <td>5.667</td>\n",
       "      <td>0.786</td>\n",
       "      <td>1.155</td>\n",
       "      <td>hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>411</td>\n",
       "      <td>謝謝樓上兩位大哥的熱心回覆我已經去原廠詢問了原因就是戶外溫度低所以空調會自動調節車內溫度後來...</td>\n",
       "      <td>6.556</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.756</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>2752</td>\n",
       "      <td>蘇丹掌權的過渡軍事委員會3日動用武力，驅散在喀土木軍事總部外靜坐抗議長達數週的群眾以來，蘇丹...</td>\n",
       "      <td>2.000</td>\n",
       "      <td>6.750</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.829</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2900 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      No.                                               Text  Valence_Mean  \\\n",
       "0    2761  呂興巨集患有老年癡呆的68歲鄖陽區老人呂興巨集，於5月22日上午從鄖陽區南化塘鎮走失後，一直...         2.889   \n",
       "1    1736  其次，不願罷免與馬總統的施政表現無直接相關，純粹只是不願意看到罷免案所引發的政治動盪，會讓臺...         3.500   \n",
       "2    1759                   美國官員表示，德黑蘭的正面迴應，暗示今年美伊舉行會談的機會增加。         5.222   \n",
       "3    2755  彌月禮盒真的很多種類，光是挑選就眼花撩亂和老公光是吃一堆油飯和蛋糕就吃得好飽後來看到明月堂可...         6.375   \n",
       "4    2742  我非常佩服姚明的一點，不僅僅是他苦練球技和體能，用一場場比賽戰績證明自己，更是高超的情商和豁...         7.000   \n",
       "..    ...                                                ...           ...   \n",
       "589  2009  可愛到爆的皮卡丘和傑尼龜、可達鴨等寶可夢一起在過去20多年裡征服的不僅是全世界的童心，還有大...         5.667   \n",
       "590  2024  5月7日，米家空調伴侶2新品開售，價格79元，讓家中傳統空調變智慧，比169元的1代便宜了一...         6.222   \n",
       "591   673                         您的入住滿意就是對我們接待服務工作最好的肯定與激勵！         6.778   \n",
       "592   411  謝謝樓上兩位大哥的熱心回覆我已經去原廠詢問了原因就是戶外溫度低所以空調會自動調節車內溫度後來...         6.556   \n",
       "593  2752  蘇丹掌權的過渡軍事委員會3日動用武力，驅散在喀土木軍事總部外靜坐抗議長達數週的群眾以來，蘇丹...         2.000   \n",
       "\n",
       "     Arousal_Mean  Valence_SD  Arousal_SD   Category  \n",
       "0           5.000       0.567       1.414       news  \n",
       "1           5.143       0.500       0.639  political  \n",
       "2           5.625       0.786       0.696  political  \n",
       "3           4.857       0.484       1.457       news  \n",
       "4           5.111       0.707       2.283       news  \n",
       "..            ...         ...         ...        ...  \n",
       "589         5.200       0.471       1.327       news  \n",
       "590         4.333       0.916       1.764       news  \n",
       "591         5.667       0.786       1.155      hotel  \n",
       "592         6.000       0.497       0.756        car  \n",
       "593         6.750       0.816       0.829       news  \n",
       "\n",
       "[2900 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fold_csv in os.listdir(data_path):\n",
    "    if fold_csv == 'CVAT_4_SD.csv':\n",
    "        df = pd.read_csv(os.path.join('data', 'CVAT_4_SD.csv'), sep=\"\\t\", encoding=\"utf-8\")\n",
    "        continue\n",
    "    cdf = pd.read_csv(os.path.join('data', fold_csv), sep=\"\\t\", encoding=\"utf-8\")\n",
    "    df = pd.concat([df, cdf])\n",
    "\n",
    "df[df['Valence_SD']<1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(csv, sm):\n",
    "    embeddings = []\n",
    "    cc = OpenCC('t2s')\n",
    "    df = pd.read_csv(os.path.join('data', csv), sep=\"\\t\", encoding=\"utf-8\")\n",
    "    for text in df['Text']:\n",
    "        text = cc.convert(text)\n",
    "        embedding = sentence_embedding(sm, text)\n",
    "        embeddings.append(embedding)\n",
    "    x = pd.DataFrame(embeddings)\n",
    "    y_v = df[f\"Valence_Mean\"]\n",
    "    y_a = df[f\"Arousal_Mean\"]\n",
    "    return [x, y_v, y_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Processing CVAT_4_SD.csv----\n",
      "\n",
      "\n",
      "----Processing CVAT_2_SD.csv----\n",
      "\n",
      "\n",
      "----Processing CVAT_5_SD.csv----\n",
      "\n",
      "\n",
      "----Processing CVAT_1_SD.csv----\n",
      "\n",
      "\n",
      "----Processing CVAT_3_SD.csv----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_datasets = []\n",
    "v_datasets = []\n",
    "a_datasets = []\n",
    "for fold_csv in os.listdir(data_path):\n",
    "    print(f\"\\n----Processing {fold_csv}----\\n\")\n",
    "    data = data_processing(fold_csv,sm)\n",
    "    x_datasets.append(data[0])\n",
    "    v_datasets.append(data[1])\n",
    "    a_datasets.append(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fold_trainer(label):\n",
    "    for fold_index in range(5):\n",
    "        x_train = pd.concat([df for index, df in enumerate(x_datasets) if index != fold_index])\n",
    "        x_test = x_datasets[fold_index]\n",
    "        if label == 'v':\n",
    "            y_train = pd.concat([df for index, df in enumerate(v_datasets) if index != fold_index])\n",
    "            y_test = v_datasets[fold_index]\n",
    "            train_dmatrix = xgb.DMatrix(x_train, label=y_train)\n",
    "            test_dmatrix = xgb.DMatrix(x_test, label=y_test)\n",
    "        else:\n",
    "            y_train = pd.concat([df for index, df in enumerate(a_datasets) if index != fold_index])\n",
    "            y_test = a_datasets[fold_index]\n",
    "            train_dmatrix = xgb.DMatrix(x_train, label=y_train)\n",
    "            test_dmatrix = xgb.DMatrix(x_test, label=y_test)\n",
    "        if fold_index == 0:\n",
    "            metric = 'mae'\n",
    "            base_params = {\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'eval_metric': metric,\n",
    "            }\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'tree_method': trial.suggest_categorical('tree_method', ['approx', 'hist']),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 250),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "                    'colsample_bynode': trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log=True),\n",
    "                    'learning_rate': 0.3,\n",
    "                }\n",
    "                pruning_callback = optuna.integration.XGBoostPruningCallback(trial, f'valid-{metric}')\n",
    "                num_boost_round = 10000\n",
    "                params.update(base_params)\n",
    "                model = xgb.train(params, train_dmatrix, num_boost_round=num_boost_round,evals=[(train_dmatrix, 'train'), (test_dmatrix, 'valid')], early_stopping_rounds=50, verbose_eval=0,\n",
    "                      callbacks=[pruning_callback])\n",
    "                # predictions = model.predict(test_dmatrix)\n",
    "                # mae = mean_absolute_error(y_test, predictions)\n",
    "                trial.set_user_attr('best_iteration', model.best_iteration)\n",
    "                return model.best_score\n",
    "            \n",
    "            sampler = optuna.samplers.TPESampler(seed=42)\n",
    "            study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "            tic = time.time()\n",
    "            while time.time() - tic < 10:\n",
    "                study.optimize(objective, n_trials=1)\n",
    "                # print('Stage 1 ==============================')\n",
    "                # print(f'best score = {study.best_trial.value}')\n",
    "                # print('boosting params ---------------------------')\n",
    "                # print(f'fixed learning rate: {0.3}')\n",
    "                # print(f'best boosting round: {study.best_trial.user_attrs[\"best_iteration\"]}')\n",
    "                # print('best tree params --------------------------')\n",
    "                for k, v in study.best_trial.params.items():\n",
    "                    print(k, ':', v)\n",
    "            # print(\"\\nkjsfjljkvsfhbskljbskfgjbsflkbfhsklb\\n\")\n",
    "\n",
    "            low_learning_rate = 0.01\n",
    "            params = {}\n",
    "            params.update(base_params)\n",
    "            params.update(study.best_trial.params)\n",
    "            params['learning_rate'] = low_learning_rate\n",
    "            model_stage2 = xgb.train(params=params, dtrain=train_dmatrix, \n",
    "                                    num_boost_round=10000,\n",
    "                                    evals=[(train_dmatrix, 'train'), (test_dmatrix, 'valid')],\n",
    "                                    early_stopping_rounds=50,\n",
    "                                    verbose_eval=0)\n",
    "            print('=======================Fold 1 ==============================')\n",
    "            print(f'best score = {mean_absolute_error(test_dmatrix.get_label(), model_stage2.predict(test_dmatrix))}')\n",
    "            print('boosting params ---------------------------')\n",
    "            print(f'fixed learning rate: {params[\"learning_rate\"]}')\n",
    "            print(f'best boosting round: {model_stage2.best_iteration}')\n",
    "            continue\n",
    "        model = xgb.train(params=params, dtrain=train_dmatrix, \n",
    "                                    num_boost_round=model_stage2.best_iteration,\n",
    "                                    evals=[(train_dmatrix, 'train'), (test_dmatrix, 'valid')],\n",
    "                                    early_stopping_rounds=50,\n",
    "                                    verbose_eval=0)\n",
    "        print(f'=========================Fold {fold_index+1} ==============================')\n",
    "        print(f'best score = {mean_absolute_error(test_dmatrix.get_label(), model.predict(test_dmatrix))}')\n",
    "        return params, model_stage2.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_full(label, params, num_boost_round):\n",
    "    x_train = pd.concat([df for index, df in enumerate(x_datasets)])\n",
    "    if label == 'v':\n",
    "        y_train = pd.concat([df for index, df in enumerate(v_datasets)])\n",
    "        train_dmatrix = xgb.DMatrix(x_train, label=y_train)\n",
    "    else:\n",
    "        y_train = pd.concat([df for index, df in enumerate(a_datasets)])\n",
    "        train_dmatrix = xgb.DMatrix(x_train, label=y_train)\n",
    "    model = xgb.train(params=params, dtrain=train_dmatrix, \n",
    "                                num_boost_round=num_boost_round,\n",
    "                                early_stopping_rounds=50,\n",
    "                                verbose_eval=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 18:17:50,948] A new study created in memory with name: no-name-114d7867-2fa7-4c99-a992-446c2d2e8ccc\n",
      "[I 2024-05-09 18:17:51,945] Trial 0 finished with value: 0.6432997764963092 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 150, 'subsample': 0.24041677639819287, 'colsample_bynode': 0.2403950683025824, 'reg_lambda': 0.001800728515054226}. Best is trial 0 with value: 0.6432997764963092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_method : hist\n",
      "max_depth : 10\n",
      "min_child_weight : 150\n",
      "subsample : 0.24041677639819287\n",
      "colsample_bynode : 0.2403950683025824\n",
      "reg_lambda : 0.001800728515054226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 18:18:10,612] Trial 1 finished with value: 0.6407127711508009 and parameters: {'tree_method': 'approx', 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.9729188669457949, 'colsample_bynode': 0.8491983767203796, 'reg_lambda': 0.008587261143813469}. Best is trial 1 with value: 0.6407127711508009.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_method : approx\n",
      "max_depth : 10\n",
      "min_child_weight : 6\n",
      "subsample : 0.9729188669457949\n",
      "colsample_bynode : 0.8491983767203796\n",
      "reg_lambda : 0.008587261143813469\n",
      "\n",
      "kjsfjljkvsfhbskljbskfgjbsflkbfhsklb\n",
      "\n",
      "Stage 2 ==============================\n",
      "best score = 0.5910483002662659\n",
      "boosting params ---------------------------\n",
      "fixed learning rate: 0.01\n",
      "best boosting round: 2513\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have at least 1 validation dataset for early stopping.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_param, baest_iteration \u001b[38;5;241m=\u001b[39m \u001b[43mFold_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m final_model \u001b[38;5;241m=\u001b[39m Train_full(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m,best_param, baest_iteration )\n",
      "Cell \u001b[0;32mIn[78], line 72\u001b[0m, in \u001b[0;36mFold_trainer\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest boosting round: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_stage2\u001b[38;5;241m.\u001b[39mbest_iteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_stage2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ==============================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_absolute_error(test_dmatrix\u001b[38;5;241m.\u001b[39mget_label(),\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(test_dmatrix))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/NLP_project/.venv/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/NLP_project/.venv/lib/python3.11/site-packages/xgboost/training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    185\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[0;32m~/Downloads/NLP_project/.venv/lib/python3.11/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(c\u001b[38;5;241m.\u001b[39mafter_iteration(model, epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Downloads/NLP_project/.venv/lib/python3.11/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Downloads/NLP_project/.venv/lib/python3.11/site-packages/xgboost/callback.py:426\u001b[0m, in \u001b[0;36mEarlyStopping.after_iteration\u001b[0;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[1;32m    424\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have at least 1 validation dataset for early stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(evals_log\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Get data name\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n",
      "\u001b[0;31mValueError\u001b[0m: Must have at least 1 validation dataset for early stopping."
     ]
    }
   ],
   "source": [
    "best_param, baest_iteration = Fold_trainer('v')\n",
    "final_model = Train_full('v',best_param, baest_iteration )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
